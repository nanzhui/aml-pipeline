{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Pipeline with ScopeStep\n",
    "This notebook is used to demonstrate the use of ScopeStep in AML Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.core.compute import ComputeTarget, DataFactoryCompute\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import AdlaStep, AzureBatchStep, DataTransferStep\n",
    "from azureml.pipeline.steps_internal import ScopeStep\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# use eastus2euap new ws\n",
    "# change the json setting back master\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(ws, 'sample_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy scope script to script folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "script_folder = './scripts'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "#shutil.copy('./failed/script.script', script_folder)\n",
    "#shutil.copy('./working/script.script', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the migrated ADLS Datastore\n",
    "For this, you will first need to assign the Azure AD application to the Azure Data Lake Storage Gen1 account file or folder. This is detailed in [this article](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-using-active-directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adl_datastore_name='MigratedADLS2'\n",
    "\n",
    "adls_datastore = Datastore.get(ws, adl_datastore_name)\n",
    "print(\"found datastore with name: %s\" % adl_datastore_name)\n",
    "\n",
    "\n",
    "# try:\n",
    "#     adls_datastore = Datastore.get(ws, adl_datastore_name)\n",
    "#     print(\"found datastore with name: %s\" % adl_datastore_name)\n",
    "# except:\n",
    "#     adls_datastore = Datastore.register_azure_data_lake(\n",
    "#         workspace=ws,\n",
    "#         datastore_name=adl_datastore_name,\n",
    "#         subscription_id=subscription_id, # subscription id of ADLS account\n",
    "#         resource_group=resource_group, # resource group of ADLS account\n",
    "#         store_name=store_name, # ADLS account name\n",
    "#         tenant_id=tenant_id, # tenant id of service principal\n",
    "#         client_id=client_id, # client id of service principal\n",
    "#         client_secret=client_secret) # the secret of service principal\n",
    "#     print(\"registered datastore with name: %s\" % adl_datastore_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = DataReference(\n",
    "    datastore=adls_datastore,\n",
    "    data_reference_name=\"InputData\",\n",
    "    #path_on_datastore=\"local/temp/juwang/input.tsv\")\n",
    "    path_on_datastore=\"local/AMLTest/input3.tsv\")\n",
    "\n",
    "output_ref = PipelineData(\"Destination\", datastore=adls_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Scope step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ScopeStep** is used to run a scope script using cosmos-migrated Azure Data Lake Analytics account.\n",
    "\n",
    "- **name:** Name of module\n",
    "- **script_name:** Name of scope script\n",
    "- **scope_param:** Parameters to pass to scope job\n",
    "- **params:** Dictionary of name-value pairs to replace in script *(optional)*\n",
    "- **custom_job_name_suffix:** Optional string to append to scope job name\n",
    "- **inputs:** List of input port bindings\n",
    "- **outputs:** List of output port bindings\n",
    "- **resources:** List of input port bindings to download resource files and substitute their local path in script\n",
    "- **adla_account_name:** the ADLA account name to use for this job\n",
    "- **source_directory:** folder that contains the script, assemblies etc. *(optional)*\n",
    "- **hash_paths:** list of paths to hash to detect a change (script file is always hashed) *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_step = ScopeStep(\n",
    "    name='Another_Script_4',\n",
    "    script_name='script.script',\n",
    "    inputs=[input_data],\n",
    "    outputs=[output_ref],\n",
    "    adla_account_name='searchrelevance-aether-test-c09', #ADLA Name, could be any ADLA name\n",
    "    allow_reuse=False,\n",
    "    source_directory=script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    description=\"Scope Script Alone 3\",\n",
    "    workspace=ws, \n",
    "    steps=[script_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = exp.submit(pipeline)\n",
    "#pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "akvasude"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:cli_dev]",
   "language": "python",
   "name": "conda-env-cli_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
