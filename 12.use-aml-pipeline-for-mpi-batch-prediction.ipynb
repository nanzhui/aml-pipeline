{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.   \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AML Pipelines for distributed batch prediction\n",
    "This notebook demonstrates how to run a distributed batch prediction job. __[Inception-V3 model](https://arxiv.org/abs/1512.00567)__  and unlabeled images from __[ImageNet](http://image-net.org/)__ dataset will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Make sure you go through the [00.aml-pipeline-configuration](./00.aml-pipeline-configuration.ipynb) Notebook first if you haven't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder that would contain python scripts that would be run on remote computes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_folder = \"scripts\"\n",
    "\n",
    "if not os.path.isdir(scripts_folder):\n",
    "    os.mkdir(scripts_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and attach Compute targets\n",
    "Use the below code to create and attach Compute targets. In this notebook we would use a 3 node batch ai cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import BatchAiCompute, ComputeTarget\n",
    "\n",
    "# Batch AI compute\n",
    "cluster_name = \"gpu-cluster\"\n",
    "try:\n",
    "    cluster = BatchAiCompute(ws, cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = BatchAiCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\",\n",
    "                                                                    autoscale_enabled=True,\n",
    "                                                                    cluster_min_nodes=0, \n",
    "                                                                    cluster_max_nodes=3)\n",
    "\n",
    "    # create the cluster\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "    cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python scripts to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_predict.py` takes input images in `dataset_path`, pretrained models in `model_dir`. Each node fills in a list called `predictions`. Using `comm.gather` gets all the `predictions` from worker nodes into a list.\n",
    "\n",
    "Eventually the aggregated predictions are written to `results-label.txt` to `outputs` directory which stores it in artifacts associated with the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $scripts_folder/batch_predict.py\n",
    "import argparse\n",
    "import datetime\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.python.slim.nets import inception_v3\n",
    "import time\n",
    "from azureml.core.model import Model\n",
    "from mpi4py import MPI\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Start a tensorflow model serving\")\n",
    "parser.add_argument('--model_dir', dest=\"model_dir\", required=True)\n",
    "parser.add_argument('--dataset_path', dest=\"dataset_path\", required=True)\n",
    "parser.add_argument('--batch_size', dest=\"batch_size\", type=int, required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "image_size = 299\n",
    "num_channel = 3\n",
    "\n",
    "# read mpi env vars to assign rank and partitions\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "    \n",
    "def get_class_label_dict(label_file):\n",
    "  label = []\n",
    "  proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
    "  for l in proto_as_ascii_lines:\n",
    "    label.append(l.rstrip())\n",
    "  return label\n",
    "\n",
    "\n",
    "class DataIterator:\n",
    "    def __init__(self, data_dir):\n",
    "        self.file_paths = []\n",
    "        image_list = sorted(os.listdir(data_dir))\n",
    "        partition_size = len(image_list) // size\n",
    "        image_list = image_list[rank * partition_size: (rank + 1) * partition_size]\n",
    "        self.file_paths = [data_dir + '/' + file_name.rstrip() for file_name in image_list ]\n",
    "\n",
    "        self.labels = [1 for file_name in self.file_paths]\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def input_pipeline(self, batch_size):\n",
    "        images_tensor = tf.convert_to_tensor(self.file_paths, dtype=tf.string)\n",
    "        labels_tensor = tf.convert_to_tensor(self.labels, dtype=tf.int64)\n",
    "        input_queue = tf.train.slice_input_producer([images_tensor, labels_tensor], shuffle=False)\n",
    "        labels = input_queue[1]\n",
    "        images_content = tf.read_file(input_queue[0])\n",
    "\n",
    "        image_reader = tf.image.decode_jpeg(images_content, channels=num_channel, name=\"jpeg_reader\")\n",
    "        float_caster = tf.cast(image_reader, tf.float32)\n",
    "        new_size = tf.constant([image_size, image_size], dtype=tf.int32)\n",
    "        images = tf.image.resize_images(float_caster, new_size)\n",
    "        images = tf.divide(tf.subtract(images, [0]), [255])\n",
    "\n",
    "        image_batch, label_batch = tf.train.batch([images, labels], batch_size=batch_size, capacity=5 * batch_size)\n",
    "        return image_batch\n",
    "\n",
    "def main(_):\n",
    "    start_time = datetime.datetime.now()\n",
    "    label_file_name = os.path.join(args.model_dir, \"labels.txt\")\n",
    "    label_dict = get_class_label_dict(label_file_name)\n",
    "    classes_num = len(label_dict)\n",
    "    test_feeder = DataIterator(data_dir=args.dataset_path)\n",
    "    total_size = len(test_feeder.labels)\n",
    "    count = 0\n",
    "    # get model from model registry\n",
    "    model_path = os.path.join(args.model_dir, \"inception_v3.ckpt\")\n",
    "    predictions = []\n",
    "    with tf.Session() as sess:\n",
    "        test_images = test_feeder.input_pipeline(batch_size=args.batch_size)\n",
    "        with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\n",
    "            input_images = tf.placeholder(tf.float32, [args.batch_size, image_size, image_size, num_channel])\n",
    "            logits, _ = inception_v3.inception_v3(input_images,\n",
    "                                                        num_classes=classes_num,\n",
    "                                                        is_training=False)\n",
    "            probabilities = tf.argmax(logits, 1)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "        i = 0\n",
    "        while count < total_size and not coord.should_stop():\n",
    "            test_images_batch = sess.run(test_images)\n",
    "            file_names_batch = test_feeder.file_paths[i*args.batch_size: min(test_feeder.size, (i+1)*args.batch_size)]\n",
    "            results = sess.run(probabilities, feed_dict={input_images: test_images_batch})\n",
    "            new_add = min(args.batch_size, total_size-count)\n",
    "            count += new_add\n",
    "            i += 1\n",
    "            for j in range(new_add):\n",
    "                predictions.append(os.path.basename(file_names_batch[j]) + \": \" + label_dict[results[j]])\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "            \n",
    "        # get results from all nodes\n",
    "        predictions_lst = comm.gather(predictions, root=0)\n",
    "        \n",
    "        # write results\n",
    "        if rank == 0:\n",
    "            out_filename = \"outputs/result-labels.txt\"\n",
    "            with open(out_filename, \"w\") as fp:\n",
    "                for preds in predictions_lst:\n",
    "                    for line in preds:\n",
    "                        fp.write(line + \"\\n\")\n",
    "                    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model and Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the public blob container `sampledata` containing images from ImageNet evaluation set and `Inception v3` model to your own storage account. Change the `\"<storage-account-name>\"` and `\"<storage-account-key>\"` to refer to the storage account's key. Change `data_container_name` and `model_container_name` to refer to your containers' names.\n",
    "\n",
    "We have shared two public containers `sample-images` and `inception-model` in a storage account `pipelinedata` that contains the data that would go in these two containers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "account_name = os.environ.get(\"PIPELINE_DATA_ACCOUNT_NAME\", \"<storage-account-name>\")\n",
    "account_key = os.environ.get(\"PIPELINE_DATA_ACCOUNT_KEY\", \"<storage-account-key\")\n",
    "\n",
    "data_container_name = \"sample-images\"\n",
    "model_container_name = \"inception-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datastores called `images_datastore` and `model_datastore` from your blob containers. The `overwrite=True` step overwrites any datastore that was created previously with that name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "images_data = Datastore.register_azure_blob_container(ws, datastore_name=\"images_datastore\", container_name=data_container_name, \n",
    "                                                        account_name=account_name, account_key=account_key, overwrite=True)\n",
    "inception_model = Datastore.register_azure_blob_container(ws, datastore_name=\"model_datastore\", container_name=model_container_name, \n",
    "                                                        account_name=account_name, account_key=account_key, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify where the input data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "input_images = DataReference(datastore=images_data, data_reference_name=\"input_images\")\n",
    "\n",
    "model_dir = DataReference(datastore=inception_model, data_reference_name=\"input_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the parameters to the python script can be given as input when we re-run a `PublishedPipeline`. In the current example, we define `batch_size` taken by the script as such parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "batch_size_param = PipelineParameter(name=\"param_batch_size\", default_value=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import MpiStep\n",
    "\n",
    "inception_model_name = \"inception_v3.ckpt\"\n",
    "\n",
    "batch_pred_step = MpiStep(\n",
    "    name=\"batch prediction\",\n",
    "    script_name=\"batch_predict.py\",\n",
    "    arguments=[\"--dataset_path\", input_images, \n",
    "               \"--model_dir\", model_dir,\n",
    "               \"--batch_size\", batch_size_param],\n",
    "    target=cluster,\n",
    "    node_count=3, \n",
    "    process_count_per_node=1,\n",
    "    inputs=[input_images, label_dir, model_dir],\n",
    "    pip_packages=[\"mpi4py\", \"tensorflow-gpu==1.10.0\"],\n",
    "    use_gpu=True,\n",
    "    source_directory=scripts_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[batch_pred_step])\n",
    "pipeline_run = Experiment(ws, 'mpi_batch_prediction').submit(pipeline, pipeline_params={\"param_batch_size\": 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and review output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_run = list(pipeline_run.get_children())[0]\n",
    "step_run.download_file(\"./outputs/result-labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"result-labels.txt\", delimiter=\":\", header=None)\n",
    "df.columns = [\"Filename\", \"Prediction\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish a pipeline and rerun using a REST call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a published pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"Inception v3 prediction\", description=\"Batch prediction using Inception v3 model\", version=\"1.0\")\n",
    "\n",
    "published_id = published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun using REST call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "import requests\n",
    "\n",
    "cli_auth = AzureCliAuthentication()\n",
    "aad_token = cli_auth.get_authentication_header()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run published pipeline using its REST endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "# specify batch size when running the pipeline\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"mpi_batch_prediction\",\n",
    "                               \"ParameterAssignments\": {\"param_batch_size\": 50}})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the new run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"mpi_batch_prediction\"], run_id)\n",
    "\n",
    "RunDetails(published_pipeline_run).show()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "hichando"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:cli_dev]",
   "language": "python",
   "name": "conda-env-cli_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
