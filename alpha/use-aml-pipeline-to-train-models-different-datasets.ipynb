{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.   \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AML Pipelines to train multiple datasets\n",
    "### Run models in parallel\n",
    "3 scripts that download and train models and are independent of each other run on the same dsvm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "# Also create a Project and attach to Workspace\n",
    "project_folder = \"scripts\"\n",
    "\n",
    "if not os.path.isdir(project_folder):\n",
    "    os.mkdir(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.compute import BatchAiCompute, ComputeTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    cpu_cluster = BatchAiCompute(ws, cpu_cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = BatchAiCompute.provisioning_configuration(vm_size = \"STANDARD_D2_v2\",\n",
    "                                                                    autoscale_enabled = True,\n",
    "                                                                    cluster_min_nodes = 3, \n",
    "                                                                    cluster_max_nodes = 3)\n",
    "\n",
    "    # create the cluster\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, provisioning_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datastore for output\n",
    "We use the default blob datastore that comes with the workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write scripts to projects directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train_regression_boston.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from azureml.core.run import Run\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train lr\")\n",
    "parser.add_argument(\"--output_dir\", type=str, help=\"output dir\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "run = Run.get_context()\n",
    "run.log(\"linear reg boston mse\", mse)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(args.output_dir, \"model.pkl\")\n",
    "with open(output_file, \"wb\") as fp:\n",
    "    pickle.dump(clf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train_rf_diabetes.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from azureml.core.run import Run\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train random forest on iris\")\n",
    "parser.add_argument(\"--output_dir\", type=str, help=\"output dir\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "run = Run.get_context()\n",
    "run.log(\"lr diabetes mse\", mse)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(args.output_dir, \"model.pkl\")\n",
    "with open(output_file, \"wb\") as fp:\n",
    "    pickle.dump(clf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train_gbt_digits.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from azureml.core.run import Run\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train gbt\")\n",
    "parser.add_argument(\"--output_dir\", type=str, help=\"output dir\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "run = Run.get_context()\n",
    "run.log(\"gbt digits accuracy\", accuracy)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(args.output_dir, \"model.pkl\")\n",
    "with open(output_file, \"wb\") as fp:\n",
    "    pickle.dump(clf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "cd = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "runconfig = RunConfiguration(conda_dependencies=cd)\n",
    "runconfig.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dir = PipelineData(\"lr_model\", datastore=default_datastore)\n",
    "rf_dir = PipelineData(\"rf_model\", datastore=default_datastore)\n",
    "gbt_dir = PipelineData(\"gbt_model\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = PythonScriptStep(\n",
    "    name=\"lr boston\",\n",
    "    script_name=\"train_regression_boston.py\",\n",
    "    arguments=[\"--output_dir\", lr_dir],\n",
    "    outputs=[lr_dir],\n",
    "    source_directory=project_folder,\n",
    "    target=cpu_cluster,\n",
    "    runconfig=runconfig\n",
    ")\n",
    "\n",
    "forest_step = PythonScriptStep(\n",
    "    name=\"rf diabetes\",\n",
    "    script_name=\"train_rf_diabetes.py\",\n",
    "    arguments=[\"--output_dir\", rf_dir],\n",
    "    outputs=[rf_dir],\n",
    "    source_directory=project_folder,\n",
    "    target=cpu_cluster,\n",
    "    runconfig=runconfig\n",
    ")\n",
    "\n",
    "gbt_step = PythonScriptStep(\n",
    "    name=\"gbt digits\",\n",
    "    script_name=\"train_gbt_digits.py\",\n",
    "    arguments=[\"--output_dir\", gbt_dir],\n",
    "    outputs=[gbt_dir],\n",
    "    source_directory=project_folder,\n",
    "    target=cpu_cluster,\n",
    "    runconfig=runconfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[lr_step, forest_step, gbt_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.validate()\n",
    "pipeline_run = Experiment(ws, \"lr_rf_gbt\").submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_run in pipeline_run.get_children():\n",
    "    print(\"{}: {}\".format(step_run.name, step_run.get_metrics()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean compute resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpu_cluster.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "hichando"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:cli_dev]",
   "language": "python",
   "name": "conda-env-cli_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
