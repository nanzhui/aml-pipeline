{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AML Pipelines to train a text dataset\n",
    "### Preprocessing 20 newsgroups text dataset to features and running several models\n",
    "This example computes numeric features for a text dataset and then runs several models on the resulting features. A step at the end then chooses the best model from the predictions done on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "# Also create a Project and attach to Workspace\n",
    "project_folder = \"scripts\"\n",
    "run_history_name = project_folder\n",
    "\n",
    "if not os.path.isdir(project_folder):\n",
    "    os.mkdir(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import BatchAiCompute, ComputeTarget\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.compute import DsvmCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch AI compute\n",
    "cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    cluster = BatchAiCompute(ws, cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = BatchAiCompute.provisioning_configuration(vm_size = \"STANDARD_D2_v2\",\n",
    "                                                                    autoscale_enabled = True,\n",
    "                                                                    cluster_min_nodes = 3, \n",
    "                                                                    cluster_max_nodes = 3)\n",
    "\n",
    "    # create the cluster\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "    cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the default blob datastore that comes with the workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/process_newsgroups.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "parser = argparse.ArgumentParser(\"generate feature hashing features from 20 newsgroups\")\n",
    "parser.add_argument(\"--out_dir\", type=str, help=\"output train dir\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.makedirs(args.out_dir)\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "X_train, X_test = data_train.data, data_test.data\n",
    "vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "obj = {}\n",
    "obj[\"X_train\"] = X_train\n",
    "obj[\"X_test\"] = X_test\n",
    "obj[\"y_train\"] = y_train\n",
    "obj[\"y_test\"] = y_test\n",
    "\n",
    "\n",
    "out_file = os.path.join(args.out_dir, \"20news.pkl\")\n",
    "with open(out_file, \"wb\") as fp:\n",
    "    pickle.dump(obj, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train_lr.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from azureml.core.run import Run\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train logistic regression on input data\")\n",
    "parser.add_argument(\"--input_dir\", type=str, help=\"input train dir\")\n",
    "parser.add_argument(\"--output_dir\", type=str, help=\"output dir\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "with open(os.path.join(args.input_dir, \"20news.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "\n",
    "X_train = obj[\"X_train\"]\n",
    "y_train = obj[\"y_train\"]\n",
    "\n",
    "X_test = obj[\"X_test\"]\n",
    "y_test = obj[\"y_test\"]\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "output_file = os.path.join(args.output_dir, \"model.pkl\")\n",
    "with open(output_file, \"wb\") as fp:\n",
    "    pickle.dump(clf, fp)\n",
    "\n",
    "accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "run = Run.get_context()\n",
    "run.log(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train_rf.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from azureml.core.run import Run\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train random classifier on input data\")\n",
    "parser.add_argument(\"--input_dir\", type=str, help=\"input train dir\")\n",
    "parser.add_argument(\"--output_dir\", type=str, help=\"output dir\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "with open(os.path.join(args.input_dir, \"20news.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "\n",
    "X_train = obj[\"X_train\"]\n",
    "X_test = obj[\"X_test\"]\n",
    "y_train = obj[\"y_train\"]\n",
    "y_test = obj[\"y_test\"]\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "output_file = os.path.join(args.output_dir, \"model.pkl\")\n",
    "with open(output_file, \"wb\") as fp:\n",
    "    pickle.dump(clf, fp)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "run = Run.get_context()\n",
    "run.log(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train_gbt.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from azureml.core.run import Run\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train random classifier on input data\")\n",
    "parser.add_argument(\"--input_dir\", type=str, help=\"input train dir\")\n",
    "parser.add_argument(\"--output_dir\", type=str, help=\"output dir\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "with open(os.path.join(args.input_dir, \"20news.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "\n",
    "X_train = obj[\"X_train\"]\n",
    "y_train = obj[\"y_train\"]\n",
    "X_test = obj[\"X_test\"]\n",
    "y_test = obj[\"y_test\"]\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "output_file = os.path.join(args.output_dir, \"model.pkl\")\n",
    "with open(output_file, \"wb\") as fp:\n",
    "    pickle.dump(clf, fp)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "run = Run.get_context()\n",
    "run.log(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "cd = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "runconfig = RunConfiguration(conda_dependencies=cd)\n",
    "runconfig.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = PipelineData(\"processed_data\", datastore=default_datastore)\n",
    "lr_model = PipelineData(\"lr\", datastore=default_datastore)\n",
    "rf_model = PipelineData(\"rf\", datastore=default_datastore)\n",
    "gbt_model = PipelineData(\"gbt\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_step = PythonScriptStep(\n",
    "    name=\"process 20newsgroups dataset\",\n",
    "    script_name=\"process_newsgroups.py\",\n",
    "    arguments=[\"--out_dir\", processed_data],\n",
    "    outputs=[processed_data],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")\n",
    "\n",
    "lr_step = PythonScriptStep(\n",
    "    name=\"train lr\",\n",
    "    script_name=\"train_lr.py\",\n",
    "    arguments=[\"--input_dir\", processed_data, \"--output_dir\", lr_model],\n",
    "    inputs=[processed_data],\n",
    "    outputs=[lr_model],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")\n",
    "\n",
    "rf_step = PythonScriptStep(\n",
    "    name=\"train rf model\",\n",
    "    script_name=\"train_rf.py\",\n",
    "    arguments=[\"--input_dir\", processed_data, \"--output_dir\", rf_model],\n",
    "    inputs=[processed_data],\n",
    "    outputs=[rf_model],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")\n",
    "\n",
    "gbt_step = PythonScriptStep(\n",
    "    name=\"train gbt\",\n",
    "    script_name=\"train_gbt.py\",\n",
    "    arguments=[\"--input_dir\", processed_data, \"--output_dir\", gbt_model],\n",
    "    inputs=[processed_data],\n",
    "    outputs=[gbt_model],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[lr_step, rf_step, gbt_step])\n",
    "pipeline.validate()\n",
    "exp = Experiment(ws, \"lr_rf_gbt\")\n",
    "pipeline_run = exp.submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor runs using widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get metrics after completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_run in pipeline_run.get_children():\n",
    "    print(\"{}: {}\".format(step_run.name, step_run.get_metrics()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean compute resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "hichando"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:cli_dev]",
   "language": "python",
   "name": "conda-env-cli_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
