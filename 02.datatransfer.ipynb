{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Pipeline with DataTranferStep\n",
    "This notebook is used to demonstrate the use of DataTranferStep in AML Pipeline.\n",
    "\n",
    "In certain cases, you will need to transfer data from one data location to another. For example, your data may be in Files storage and you may want to move it to Blob storage. Or, if your data is in an ADLS account and you want to make it available in the Blob storage. Built-in **DataTransferStep** can help with such movement of data.\n",
    "\n",
    "The below example shows how to move data in an ADLS account to the Blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML and Pipeline SDK-specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import AdlaStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core import attach_legacy_compute_target\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<my-subscription-id>\"\n",
    "resource_group = \"<my-rg>\"\n",
    "store_name = \"<my-sotrename>\"\n",
    "tenant_id = \"<my-tenant>\"\n",
    "client_id = \"<my-client-id>\"\n",
    "client_secret = \"<my-client-secret>\"\n",
    "\n",
    "adls_datastore = Datastore.register_azure_data_lake(\n",
    "    workspace=ws,\n",
    "    datastore_name='MyAdlsDatastore',\n",
    "    subscription_id=subscription_id, # subscription id of ADLS account\n",
    "    resource_group=resource_group, # resource group of ADLS account\n",
    "    store_name=store_name, # ADLS account name\n",
    "    tenant_id=tenant_id, # tenant id of service principal\n",
    "    client_id=client_id, # client id of service principal\n",
    "    client_secret=client_secret) # the secret of service principal\n",
    "      \n",
    "blob_datastore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws,\n",
    "    datastore_name='MyBlobDatastore',\n",
    "    account_name=\"<my-account>\", # Storage account name\n",
    "    container_name=\"<my-container>\", # Name of Azure blob container\n",
    "    account_key=\"<my-account-key>\") # Storage account key\n",
    "\n",
    "# CLI:\n",
    "# az ml datastore register-blob -n <datastore-name> -a <account-name> -c <container-name> -k <account-key> [-t <sas-token>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataReferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adls_datastore = Datastore(workspace=ws, name=\"MyAdlsDatastore\")\n",
    "adls_data_ref = DataReference(\n",
    "    datastore=adls_datastore,\n",
    "    data_reference_name=\"adls_test_data\",\n",
    "    path_on_datastore=\"testdata\")\n",
    "\n",
    "blob_datastore = Datastore(workspace=ws, name=\"MyBlobDatastore\")\n",
    "blob_data_ref = DataReference(\n",
    "    datastore=blob_datastore,\n",
    "    data_reference_name=\"blob_test_data\",\n",
    "    path_on_datastore=\"testdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Factory Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factory_name = 'adftest'\n",
    "\n",
    "def get_or_create_data_factory(workspace, factory_name):\n",
    "    try:\n",
    "        return DataFactoryCompute(workspace, factory_name)\n",
    "    except ComputeTargetException as e:\n",
    "        if 'ComputeTargetNotFound' in e.message:\n",
    "            print('Data factory not found, creating...')\n",
    "            provisioning_config = DataFactoryCompute.provisioning_configuration()\n",
    "            data_factory = ComputeTarget.create(workspace, factory_name, provisioning_config)\n",
    "            data_factory.wait_for_provisioning()\n",
    "            return data_factory\n",
    "        else:\n",
    "            raise e\n",
    "            \n",
    "data_factory_compute = get_or_create_data_factory(ws, data_factory_name)\n",
    "\n",
    "# CLI:\n",
    "# Create: az ml computetarget setup datafactory -n <name>\n",
    "# BYOC: az ml computetarget attach datafactory -n <name> -i <resource-id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataTransferStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataTransferStep: Transfers data between Azure Blob and Data Lake accounts.\n",
    "\n",
    "- **name:** Name of module\n",
    "- **source_data_reference:** Input connection that serves as source of data transfer operation.\n",
    "- **destination_data_reference:** Input connection that serves as destination of data transfer operation.\n",
    "- **data_factory_compute:** Azure Data Factory to use for transferring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_adls_to_blob = DataTransferStep(\n",
    "    name=\"transfer_adls_to_blob\",\n",
    "    source_data_reference=adls_data_ref,\n",
    "    destination_data_reference=blob_data_ref,\n",
    "    data_factory_compute=data_factory_compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    description=\"data_transfer_101\",\n",
    "    workspace=ws,\n",
    "    steps=[transfer_adls_to_blob])\n",
    "\n",
    "pipeline_run = Experiment(ws, run_history_name).submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the run\n",
    "You can cycle through the node_run objects and examine job logs, stdout, and stderr of each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_runs = pipeline_run.get_children()\n",
    "for step_run in step_runs:\n",
    "    status = step_run.get_status()\n",
    "    print('node', step_run.name, 'status:', status)\n",
    "    if status == \"Failed\":\n",
    "        joblog = step_run.get_job_log()\n",
    "        print('job log:', joblog)\n",
    "        stdout_log = step_run.get_stdout_log()\n",
    "        print('stdout log:', stdout_log)\n",
    "        stderr_log = step_run.get_stderr_log()\n",
    "        print('stderr log:', stderr_log)\n",
    "        with open(\"logs-\" + step_run.name + \".txt\", \"w\") as f:\n",
    "            f.write(joblog)\n",
    "            print(\"Job log written to logs-\"+ step_run.name + \".txt\")\n",
    "    if status == \"Finished\":\n",
    "        stdout_log = step_run.get_stdout_log()\n",
    "        print('stdout log:', stdout_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
