{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Pipeline with DatabricksStep\n",
    "To use Databricks as a compute target from [AML Pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines), a [DatabricksStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py) is used. This notebook demonstrates the use of DatabricksStep in AML Pipeline.\n",
    "\n",
    "## Databricks as a Compute Target\n",
    "### Already available\n",
    "1.\tRunning an arbitrary Databricks notebook that the customer has at her Databricks workspace (it can take inputs/arguments and can produce outputs. You can also chain inputs and outputs using Azure Blob and ADLS Data Stores)\n",
    "2.\tRunning an arbitrary Python script that the customer has in dbfs for the ADB workspace (it can take inputs/arguments and can produce outputs)\n",
    "3.\tDatabricks method of specifying library dependencies\n",
    "\n",
    "### Coming soon\n",
    "1.\tRunning a JAR job that the customer has in dbfs for the ADB workspace (it can take inputs/arguments and can produce outputs)\n",
    "2.\tAdding dependent data files to run the specific job through DatabricksStep. Will support uploading files and scripts required for a job to dbfs for the ADB workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML and Pipeline SDK-specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and attach Compute targets\n",
    "1. You need to create an Azure Databricks workspace in the same subscription as your AML workspace or use an existing one. [Click here](https://ms.portal.azure.com/#blade/HubsExtension/Resources/resourceType/Microsoft.Databricks%2Fworkspaces) for more information.\n",
    "2. Next, you need to add your Databricks workspace to AML as a compute target and give it a name. You will use this name to refer to your Databricks workspace compute target inside AML.\n",
    "\n",
    "  - **databricks_compute_name**: The name you choose for your databricks compute target in AML.\n",
    "  - **databricks_resource_id**: The Azure Resource Id for your Databricks workspace. This is in the form: `/subscriptions/b8c23406-f9b5-4ccb-8a65-a8cb5dcd6a5a/resourceGroups/alondatabricksrg/providers/Microsoft.Databricks/workspaces/alondatabricks`\n",
    "  - **databricks_access_token**: You need to manually create a Databricks access token by connecting to you Databricks workspace in a web browser and copy its value here. See [this](https://docs.databricks.com/api/latest/authentication.html#generate-a-token) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databricks_compute_name = \"<databricks_compute_name>\"\n",
    "databricks_resource_id = \"<databricks_resource_id>\"\n",
    "databricks_access_token = \"<databricks_access_token>\"\n",
    "\n",
    "try:\n",
    "    databricks_compute = ComputeTarget(workspace=ws, name=databricks_compute_name)\n",
    "    print('Compute target already exists')\n",
    "except ComputeTargetException:\n",
    "    print('compute not found')\n",
    "    print('databricks_compute_name {}'.format(databricks_compute_name))\n",
    "    print('databricks_resource_id {}'.format(databricks_resource_id))\n",
    "    print('databricks_access_token {}'.format(databricks_access_token))\n",
    "    databricks_compute = DatabricksCompute.attach(\n",
    "             workspace=ws,\n",
    "             name=databricks_compute_name,\n",
    "             resource_id=databricks_resource_id,\n",
    "             access_token=databricks_access_token\n",
    "         )\n",
    "\n",
    "    databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will need\n",
    "### Sample Databricks notebook to test functionality\n",
    "\n",
    "```python\n",
    "dbutils.widgets.get(\"myparam\")\n",
    "p = getArgument(\"myparam\")\n",
    "print (\"Param -\\'myparam':\")\n",
    "print (p)\n",
    "\n",
    "dbutils.widgets.get(\"input\")\n",
    "i = getArgument(\"input\")\n",
    "print (\"Param -\\'input':\")\n",
    "print (i)\n",
    "\n",
    "dbutils.widgets.get(\"output\")\n",
    "o = getArgument(\"output\")\n",
    "print (\"Param -\\'output':\")\n",
    "print (o)\n",
    "\n",
    "n = i + \"/test\"\n",
    "df = spark.read.csv(n)\n",
    "\n",
    "display (df)\n",
    "\n",
    "data = [('value1', 'value2')]\n",
    "df2 = spark.createDataFrame(data)\n",
    "\n",
    "z = o + \"/test\"\n",
    "df2.write.csv(z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Connections with Inputs and Outputs\n",
    "\n",
    "#### Interacting with inputs and outputs from a Databricks notebook\n",
    "The Databricks step supports Azure Blob and ADLS Datastore inputs and outputs. You have two ways to interact with the inputs and outputs from your Databricks notebook: \n",
    "1. [Azure Blob Storage](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-storage.html)\n",
    "2. [Azure Data Lake Storage](https://docs.databricks.com/spark/latest/data-sources/azure/azure-datalake.html)\n",
    "\n",
    "\n",
    "#### Direct Data Access\n",
    "Databricks allows you to interact with Azure Blob or ADLS URIs directly. The input or output uris will be mapped to a Databricks widget param in the Databricks notebook. So if you have a data reference named \"myinput\" it will represent the URI of the input and you can access it directly in the Databricks python notebook like so:\n",
    "\n",
    "```python\n",
    "dbutils.widgets.get(\"myinput\")\n",
    "y = getArgument(\"myinput\")\n",
    "df = spark.read.csv(y)\n",
    "```\n",
    "\n",
    "#### Mounting\n",
    "You will be supplied with additional parameters and secrets that will enable you to mount your ADLS or Azure Blob input or output location in your Databricks notebook.\n",
    "\n",
    "##### Azure Blob Mounting\n",
    "Given an Azure Blob data reference named \"myinput\" the following widget params will be made available in the Databricks notebook:\n",
    "\n",
    "```python\n",
    "dbutils.widgets.get(\"myinput\") # which will contain the input URI\n",
    "dbutils.widgets.get(\"myinput_blob_secretname\") # which will contain the name of a Databricks secret (in the predefined \"amlscope\" secret scope) that contians an access key or sas for the Azure Blob input\n",
    "dbutils.widgets.get(\"input_blob_config\") - which will contain the required configuration for mounting\n",
    "```\n",
    "\n",
    "You can bring it all together to mount the Azure Blob like so:\n",
    "\n",
    "```python\n",
    "dbutils.widgets.get(\"myinput\")\n",
    "myinput_uri = getArgument(\"myinput\")\n",
    "\n",
    "dbutils.widgets.get(\"myinput_blob_secretname\")\n",
    "myinput_blob_secretname = getArgument(\"myinput_blob_secretname\")\n",
    "\n",
    "dbutils.widgets.get(\"myinput_blob_config\")\n",
    "myinput_blob_config = getArgument(\"myinput_blob_config\")\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = myinput_uri,\n",
    "  mount_point = \"/mnt/input\",\n",
    "  extra_configs = {myinput_blob_config:dbutils.secrets.get(scope = \"amlscope\", key = myinput_blob_secretname)})\n",
    "```  \n",
    "  \n",
    "##### ADLS Mounting\n",
    "Given an ADLS data reference named \"myinput\" the following widget params will be made available in the Databricks notebook:\n",
    "\n",
    "```python\n",
    "dbutils.widgets.get(\"myinput\") # which will contain the input URI\n",
    "dbutils.widgets.get(\"myinput__adls_clientid\") # which will contain the client id for the service principal that has access to the adls input\n",
    "dbutils.widgets.get(\"myinput_adls_secretname\") # which will contain the name of a Databricks secret (in the predefined \"amlscope\" secret scope) that contains the secret for the above mentioned service principal\n",
    "dbutils.widgets.get(\"myinput_adls_refresh_url\") # which will contain the refresh url for the mounting configs\n",
    "```\n",
    "\n",
    "You can bring it all together to mount ADLS like this:\n",
    "\n",
    "```python\n",
    "dbutils.widgets.get(\"myinput\")\n",
    "myinput_uri = getArgument(\"myinput\")\n",
    "\n",
    "dbutils.widgets.get(\"myinput_adls_clientid\")\n",
    "myinput_adls_clientid = getArgument(\"myinput_adls_clientid\")\n",
    "\n",
    "dbutils.widgets.get(\"myinput_adls_secretname\")\n",
    "myinput_adls_secretname = getArgument(\"myinput_adls_secretname\")\n",
    "\n",
    "dbutils.widgets.get(\"myinput_adls_refresh_url\")\n",
    "myinput_adls_refresh_url = getArgument(\"myinput_adls_refresh_url\")\n",
    "\n",
    "configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n",
    "           \"dfs.adls.oauth2.client.id\": myinput_adls_clientid,\n",
    "           \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \"amlscope\", key =myinput_adls_secretname),\n",
    "           \"dfs.adls.oauth2.refresh.url\": myinput_adls_refresh_url}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = myinput_uri,\n",
    "  mount_point = \"/mnt/output\",\n",
    "  extra_configs = configs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Databricks from AML Pipeline\n",
    "### Create/Register a Datastore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to programmatically create a Datastore from an Azure blob. Fill the relevant values below.\n",
    "datastore_name = \"<datastore_name>\"\n",
    "container_name = \"<container_name>\"\n",
    "storage_account = \"<storage_account>\"\n",
    "storage_account_key = \"<storage_account_key>\"\n",
    "\n",
    "try:\n",
    "    ds = Datastore.get(workspace=ws, datastore_name=datastore_name)\n",
    "    print('Datastore already exists')\n",
    "except:\n",
    "    print('Creating datastore')\n",
    "    ds = Datastore.register_azure_blob_container(ws, datastore_name, container_name,\n",
    "                                                 account_name=storage_account, account_key=storage_account_key,\n",
    "                                                 create_if_not_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a DatabricksStep\n",
    "Adds a Databricks notebook as a step in a Pipeline.\n",
    "- ***name:** Name of the Module\n",
    "- **inputs:** List of input connections for data consumed by this step. Fetch this inside the notebook using dbutils.widgets.get(\"input\")\n",
    "- **outputs:** List of output port definitions for outputs produced by this step. Fetch this inside the notebook using dbutils.widgets.get(\"output\")\n",
    "- **spark_version:** Version of spark for the databricks run cluster. default value: 4.0.x-scala2.11\n",
    "- **node_type:** Azure vm node types for the databricks run cluster. default value: Standard_D3_v2\n",
    "- **num_workers:** Number of workers for the databricks run cluster\n",
    "- **autoscale:** The autoscale configuration for the databricks run cluster\n",
    "- **spark_env_variables:** Spark environment variables for the databricks run cluster (dictionary of {str:str}). default value: {'PYSPARK_PYTHON': '/databricks/python3/bin/python3'}\n",
    "- ***notebook_path:** Path to the notebook in the databricks instance.\n",
    "- **notebook_params:** Parameters  for the databricks notebook (dictionary of {str:str}). Fetch this inside the notebook using dbutils.widgets.get(\"myparam\")\n",
    "- **run_name:** Name in databricks for this run\n",
    "- **timeout_seconds:** Timeout for the databricks run\n",
    "- **maven_libraries:** maven libraries for the databricks run\n",
    "- **pypi_libraries:** pypi libraries for the databricks run\n",
    "- **egg_libraries:** egg libraries for the databricks run\n",
    "- **jar_libraries:** jar libraries for the databricks run\n",
    "- **rcran_libraries:** rcran libraries for the databricks run\n",
    "- **databricks_compute:** Azure Databricks compute\n",
    "- **databricks_compute_name:** Name of Azure Databricks compute\n",
    "\n",
    "\\* *denotes required fields*  \n",
    "*You must provide exactly one of num_workers or autoscale paramaters*  \n",
    "*You must provide exactly one of databricks_compute or databricks_compute_name parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_input = DataReference(datastore=ds, path_on_datastore=\"test\",\n",
    "                                     data_reference_name=\"input\")\n",
    "\n",
    "step_1_output = PipelineData(\"output\", datastore_name=datastore_name)\n",
    "\n",
    "notebook_path = os.environ.get(\"AML_DATABRICKS_NOTEBOOK_PATH\", \"<databricks_notebook_path>\")\n",
    "\n",
    "dbStep = DatabricksStep(\n",
    "    name=\"databricksmodule\",\n",
    "    inputs=[step_1_input],\n",
    "    outputs=[step_1_output],\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    notebook_params={'myparam': 'testparam'},\n",
    "    run_name='demo run name',\n",
    "    databricks_compute=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of steps to run\n",
    "steps = [dbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'Databricks demo experiment').submit(pipeline)\n",
    "#pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: ADLA as a Compute Target\n",
    "To use ADLA as a compute target from AML Pipeline, a AdlaStep is used. This [notebook](./06.use-adla-as-compute-target.ipynb) demonstrates the use of AdlaStep in AML Pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
