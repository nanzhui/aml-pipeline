{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use AML Pipelines to train multiple featurizers\n",
    "### Multiple features on the same dataset, concatenate, and train\n",
    "Using the 20newsgroups dataset as an example, we will first compute features on the same dataset using two different featurizers. For this demo we use the same machine, however in cases with some featurizers being more expensive than others and large datasets it might make sense to split this to different machines.\n",
    "Eventually, features from each of these are concatenated and used to train a `sklearn` `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "# Also create a Project and attach to Workspace\n",
    "project_folder = \"scripts\"\n",
    "run_history_name = project_folder\n",
    "\n",
    "if not os.path.isdir(project_folder):\n",
    "    os.mkdir(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import BatchAiCompute, ComputeTarget\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.compute import DsvmCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch AI compute\n",
    "cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    cluster = BatchAiCompute(ws, cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = BatchAiCompute.provisioning_configuration(vm_size = \"STANDARD_D2_v2\",\n",
    "                                                                    autoscale_enabled = True,\n",
    "                                                                    cluster_min_nodes = 3, \n",
    "                                                                    cluster_max_nodes = 3)\n",
    "\n",
    "    # create the cluster\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "    cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the default blob datastore that comes with the workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python scripts\n",
    "- `fetch_newsgroups.py`: Fetch 20newsgroups data\n",
    "- `hashing_features.py`: Use feature hashing to generate features\n",
    "- `tfidf_features.py`: Compute tfidf features\n",
    "- `train_model.py`: Concatenate and train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/fetch_newsgroups.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "parser = argparse.ArgumentParser(\"download 20 newsgroups dataset\")\n",
    "parser.add_argument(\"--out_dir\", type=str, help=\"output data dir\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "obj = {}\n",
    "obj[\"data_train\"] = data_train\n",
    "obj[\"data_test\"] = data_test\n",
    "\n",
    "os.makedirs(args.out_dir)\n",
    "\n",
    "with open(os.path.join(args.out_dir, \"20news.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(obj, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/hashing_features.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "parser = argparse.ArgumentParser(\"generate feature hashing features for 20 newsgroups\")\n",
    "parser.add_argument(\"--input_dir\", type=str, help=\"data directory\")\n",
    "parser.add_argument(\"--out_dir\", type=str, help=\"output feature hashing features directory\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False)\n",
    "\n",
    "with open(os.path.join(args.input_dir, \"20news.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "    \n",
    "data_train = obj[\"data_train\"]\n",
    "    \n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "\n",
    "obj = {}\n",
    "obj[\"X_train\"] = X_train\n",
    "obj[\"vectorizer\"] = vectorizer\n",
    "\n",
    "os.makedirs(args.out_dir)\n",
    "\n",
    "with open(os.path.join(args.out_dir, \"feats.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(obj, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/tfidf_features.py\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "parser = argparse.ArgumentParser(\"generate feature hashing features for 20 newsgroups\")\n",
    "parser.add_argument(\"--input_dir\", type=str, help=\"data directory\")\n",
    "parser.add_argument(\"--out_dir\", type=str, help=\"output tfidf features directory\")\n",
    "parser.add_argument(\"--ngram\", type=int, help=\"character ngram length\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(args.ngram, args.ngram), analyzer=\"char\")\n",
    "\n",
    "with open(os.path.join(args.input_dir, \"20news.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "    \n",
    "data_train = obj[\"data_train\"]\n",
    "\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "\n",
    "obj = {}\n",
    "obj[\"X_train\"] = X_train\n",
    "obj[\"vectorizer\"] = vectorizer\n",
    "\n",
    "os.makedirs(args.out_dir)\n",
    "with open(os.path.join(args.out_dir, \"feats.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(obj, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train_model.py\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from azureml.core.run import Run\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train model for 20 newsgroups\")\n",
    "parser.add_argument(\"--hashing_dir\", type=str, help=\"feature hashing directory\")\n",
    "parser.add_argument(\"--tfidf_dir\", type=str, help=\"tfidf features directory\")\n",
    "parser.add_argument(\"--input_dir\", type=str, help=\"data directory\")\n",
    "parser.add_argument(\"--output_dir\", type=str, help=\"output model dir\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "vectorizers = []\n",
    "X_train = []\n",
    "\n",
    "with open(os.path.join(args.hashing_dir, \"feats.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "    vectorizers.append((\"feature_hashing\", obj[\"vectorizer\"]))\n",
    "    X_train.append(obj[\"X_train\"])\n",
    "    \n",
    "with open(os.path.join(args.tfidf_dir, \"feats.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "    vectorizers.append((\"tfidf_features\", obj[\"vectorizer\"]))\n",
    "    X_train.append(obj[\"X_train\"])\n",
    "    \n",
    "with open(os.path.join(args.input_dir, \"20news.pkl\"), \"rb\") as fp:\n",
    "    obj = pickle.load(fp)\n",
    "    y_train = obj[\"data_train\"].target\n",
    "    y_test = obj[\"data_test\"].target\n",
    "    raw_X_test = obj[\"data_test\"].data\n",
    "    \n",
    "X_train = sparse.hstack(X_train)\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "final_model = sklearn.pipeline.Pipeline([(\"transformer\", \n",
    "                                          sklearn.pipeline.FeatureUnion(vectorizers)), \n",
    "                                         (\"model\", lr_model)])\n",
    "\n",
    "# check performance of final model\n",
    "pred_probs = final_model.predict_proba(raw_X_test)\n",
    "\n",
    "# binarize labels to compute average auc\n",
    "binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "binarizer.fit(y_train)\n",
    "y_test_bin = binarizer.transform(y_test)\n",
    "auc = roc_auc_score(y_test_bin, pred_probs)\n",
    "print(f\"Current AUC: {auc}\")\n",
    "\n",
    "run = Run.get_context()\n",
    "run.log(\"auc\", auc)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "out_file = os.path.join(args.output_dir, \"model.pkl\")\n",
    "with open(out_file, \"wb\") as fp:\n",
    "    pickle.dump(final_model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define runconfig environment in the dsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "cd = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "runconfig = RunConfiguration(conda_dependencies=cd)\n",
    "runconfig.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipelineData where the code is written to and read from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = PipelineData(\"rawdata\", datastore=default_datastore)\n",
    "hashing_features = PipelineData(\"hashing\", datastore=default_datastore)\n",
    "tfidf_features = PipelineData(\"tfidf\", datastore=default_datastore)\n",
    "output_dir = PipelineData(\"model_output\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define steps and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_step = PythonScriptStep(\n",
    "    name=\"fetch 20newsgroups dataset\",\n",
    "    script_name=\"fetch_newsgroups.py\",\n",
    "    arguments=[\"--out_dir\", raw_data],\n",
    "    outputs=[raw_data],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")\n",
    "\n",
    "feature_hashing_step = PythonScriptStep(\n",
    "    name=\"feature hashing\",\n",
    "    script_name=\"hashing_features.py\",\n",
    "    arguments=[\"--input_dir\", raw_data, \"--out_dir\", hashing_features],\n",
    "    inputs=[raw_data],\n",
    "    outputs=[hashing_features],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")\n",
    "\n",
    "tfidf_step = PythonScriptStep(\n",
    "    name=\"tfidf\",\n",
    "    script_name=\"tfidf_features.py\",\n",
    "    arguments=[\"--input_dir\", raw_data, \"--out_dir\", tfidf_features, \"--ngram\", 3],\n",
    "    inputs=[raw_data],\n",
    "    outputs=[tfidf_features],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")\n",
    "\n",
    "model_step = PythonScriptStep(\n",
    "    name=\"train the final model\",\n",
    "    script_name=\"train_model.py\",\n",
    "    arguments=[\"--input_dir\", raw_data,\n",
    "               \"--hashing_dir\", hashing_features,\n",
    "               \"--tfidf_dir\", tfidf_features,\n",
    "               \"--output_dir\", output_dir\n",
    "              ],\n",
    "    inputs=[raw_data, hashing_features, tfidf_features],\n",
    "    outputs=[output_dir],\n",
    "    source_directory=project_folder,\n",
    "    runconfig=runconfig,\n",
    "    target=cluster\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[model_step])\n",
    "pipeline.validate()\n",
    "pipeline_run = Experiment(ws, \"train_model_20newsgroups\").submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor runs using widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete run and print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()\n",
    "for step_run in pipeline_run.get_children():\n",
    "    print(\"{}: {}\".format(step_run.name, step_run.get_metrics()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally Clean compute resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "hichando"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:cli_dev]",
   "language": "python",
   "name": "conda-env-cli_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
